{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+tRDzqftRrEZRHKYKI9UY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UsmanQT/AI-advisory-system/blob/generate-embeddings-alpaca-colab/Load_Embeddings_Generate_Question_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of this notebook deals with reading the embeddings stored in the firebase collection \"paragraphs\" and computing the cosine similarity of the question's embedding and the embeddings read from the collection. We will get the top matched paragraphs and concatinate those paragraphs to make the context. In the second part of the notebook, we will use that context to answer the question."
      ],
      "metadata": {
        "id": "-JILszAvV2qE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgjD08yOG9tL"
      },
      "outputs": [],
      "source": [
        "!pip install firebase\n",
        "!pip install firebase-admin\n",
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import firebase_admin\n",
        "import numpy as np\n",
        "from langchain import HuggingFaceHub\n",
        "import os\n",
        "import requests\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7vcK1V-uHUGd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting some environment variables, tokens and global variables\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "hf_token = \"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\""
      ],
      "metadata": {
        "id": "l6w2wYuAHXhX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get reference of the database where the embeddings are stored\n",
        "def initializeFirebase():\n",
        "  cred = credentials.Certificate(\"ADD-YOUR-JSON-FILE.json\")\n",
        "  firebase_admin.initialize_app(cred)\n",
        "\n",
        "  # Get a reference to your Firestore database\n",
        "  db = firestore.client()"
      ],
      "metadata": {
        "id": "jDEscIWRq7xZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to compute cosine similarity\n",
        "def cosine_similarity(embedding1, embedding2):\n",
        "    # Assuming embeddings are numpy arrays\n",
        "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))"
      ],
      "metadata": {
        "id": "PvCaqnQiHahr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the extract the context from similar paragraphs\n",
        "def getSimilarParagraphs(modelName, askQuestion):\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=modelName)\n",
        "  question = askQuestion\n",
        "  question_embedding = embeddings.embed_documents(question)\n",
        "  print(\"Question: \")\n",
        "  print(question)\n",
        "  question_embedding = np.array(question_embedding)\n",
        "  print(\"Question's embeddings: \")\n",
        "  print(question_embedding)\n",
        "\n",
        "  # Initialize an empty list to store the answers\n",
        "  answers_list = []\n",
        "  # Initialize a list to store (similarity, text) pairs\n",
        "  results = []\n",
        "\n",
        "  # Query the \"paragraphs\" collection and retrieve the \"answer\" field from each document\n",
        "  paragraphs_ref = db.collection(\"paragraphs\")\n",
        "  docs = paragraphs_ref.stream()\n",
        "\n",
        "  for doc in docs:\n",
        "      doc_data = doc.to_dict()\n",
        "      paragraph_text = doc_data['text']\n",
        "      if 'embeddings' in doc_data and 'text' in doc_data and len(paragraph_text) >= 70:\n",
        "        answer_embeddings = np.array(doc_data['embeddings'])\n",
        "\n",
        "        similarity = cosine_similarity(question_embedding, answer_embeddings)\n",
        "\n",
        "        results.append((similarity, doc_data['text']))\n",
        "  # Sort the results by similarity in descending order\n",
        "  results.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  # Return the top similar paragraphs\n",
        "  n = 10  # Number of similar paragraphs to retrieve\n",
        "  top_n_results = results[:n]\n",
        "\n",
        "  print(f\"Top {n} paragraphs: \")\n",
        "  print(top_n_results)\n",
        "\n",
        "  context = \"\"\n",
        "  for i in top_n_results:\n",
        "    context = context + i[1].strip()\n",
        "  context = context.replace('\\n',' ')\n",
        "  return context"
      ],
      "metadata": {
        "id": "9PluQVJ8HsoE"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function call to get the reference of database\n",
        "initializeFirebase()"
      ],
      "metadata": {
        "id": "yEamVaZHW1Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function call to get the context for the question from the similar paragraphs that are stored in firebase database.\n",
        "question=[\"who is Jonathan Engelsma?\"]\n",
        "generatedContext = getSimilarParagraphs(modelName=\"declare-lab/flan-alpaca-large\", askQuestion = question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvG4gG_SIXyZ",
        "outputId": "a8c4efe4-2a84-4c5d-bba1-010739e694d3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/declare-lab_flan-alpaca-large. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: \n",
            "['who is Jonathan Engelsma?']\n",
            "Question's embeddings: \n",
            "[[-0.01108511  0.00080256  0.02336332 ...  0.04015426 -0.00619658\n",
            "  -0.0211643 ]]\n",
            "Top 10 paragraphs: \n",
            "[(array([0.34684686]), 'Any questions regarding senior projects, can be directed to the current instructors of the course: Dr. Adams and/or Dr. Engelsma.'), (array([0.34323968]), 'Dr. Kalafutâ€™s teaching and research focuses on networking and security. \\xa0He completed his Ph.D. in computer science at Indiana University in 2010, where he focused on cyberfraud detection through infrastructure analysis.'), (array([0.33984828]), 'Undergraduates interested in applying for an ACI Residency should email their CV to Dr. Engelsma at least one semester in advance, indicating their interest in the program.'), (array([0.32985065]), '\\nPoint of contact:\\xa0 Email Rahat Rafiq ( rafiqr@gvsu.edu ) if you have any queries.'), (array([0.32528429]), '\\n\\t\\t\\t\\t\\t\\t\\tCome and attend the talk by Dr. Mike Doyle titled \"The Visible\\nEmbryo Project: Following the connections from chick embryos to Bitcoin.\\n\\t\\t\\t\\t\\t\\t'), (array([0.32488622]), 'Dr. Zachary DeBruine is an Assistant Professor of Computing at Grand Valley State University within the Applied Computing Institute. Dr. DeBruine is leading research in collaboration with academic and industry partners to develop high-performance machine learning algorithms to analyze big biological data.\\n\\xa0'), (array([0.32050788]), '\\n\\t\\t\\t\\t\\t\\t\\tDr. Rafiq Presents Research on Macro-Level Sentiments on Twitter in\\nLima, Peru\\n\\t\\t\\t\\t\\t\\t'), (array([0.32016012]), 'Usman Tahir Qureshi is currently a Full-stack Developer and also engaged in research related to conversational AI and Machine Learning. He is currently pursuing a MS in Applied Computer Science at GVSU.'), (array([0.3136183]), '\\n\\t\\t\\t\\t\\t\\t\\tCome and attend the talk by Dr. Joshua Engelsma, where he sheds light on\\nthe recent trends and challenges in the area of Automated Fingerprint\\nIdentification Systems (AFIS) and how recent advances in computer vision\\ncan tackle some of those challenges.\\n\\t\\t\\t\\t\\t\\t'), (array([0.3136183]), '\\n\\t\\t\\t\\t\\t\\t\\tCome and attend the talk by Dr. Joshua Engelsma, where he sheds light on\\nthe recent trends and challenges in the area of Automated Fingerprint\\nIdentification Systems (AFIS) and how recent advances in computer vision\\ncan tackle some of those challenges.\\n\\t\\t\\t\\t\\t\\t')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generatedContext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "qaZqS5QGZKR0",
        "outputId": "64e8d5ce-befa-4d1a-d59b-c0aacfaabdc7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Any questions regarding senior projects, can be directed to the current instructors of the course: Dr. Adams and/or Dr. Engelsma.Dr. Kalafutâ€™s teaching and research focuses on networking and security. \\xa0He completed his Ph.D. in computer science at Indiana University in 2010, where he focused on cyberfraud detection through infrastructure analysis.Undergraduates interested in applying for an ACI Residency should email their CV to Dr. Engelsma at least one semester in advance, indicating their interest in the program.Point of contact:\\xa0 Email Rahat Rafiq ( rafiqr@gvsu.edu ) if you have any queries.Come and attend the talk by Dr. Mike Doyle titled \"The Visible Embryo Project: Following the connections from chick embryos to Bitcoin.Dr. Zachary DeBruine is an Assistant Professor of Computing at Grand Valley State University within the Applied Computing Institute. Dr. DeBruine is leading research in collaboration with academic and industry partners to develop high-performance machine learning algorithms to analyze big biological data.Dr. Rafiq Presents Research on Macro-Level Sentiments on Twitter in Lima, PeruUsman Tahir Qureshi is currently a Full-stack Developer and also engaged in research related to conversational AI and Machine Learning. He is currently pursuing a MS in Applied Computer Science at GVSU.Come and attend the talk by Dr. Joshua Engelsma, where he sheds light on the recent trends and challenges in the area of Automated Fingerprint Identification Systems (AFIS) and how recent advances in computer vision can tackle some of those challenges.Come and attend the talk by Dr. Joshua Engelsma, where he sheds light on the recent trends and challenges in the area of Automated Fingerprint Identification Systems (AFIS) and how recent advances in computer vision can tackle some of those challenges.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next part of the notebook deals with using a question/answer open source machine learning model. We will feed the model the question and the context (generated in the first part) and get the answer."
      ],
      "metadata": {
        "id": "mg_R7QerVlR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIRST APPROACH USING consciousAI/question-answering model"
      ],
      "metadata": {
        "id": "We1cwlXtbIPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer)"
      ],
      "metadata": {
        "id": "zSb9zbvSoP_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate(query, context, model, device):\n",
        "\n",
        "    FT_MODEL = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)\n",
        "    FT_MODEL_TOKENIZER = AutoTokenizer.from_pretrained(model)\n",
        "    input_text = \"Extract the answer from question_context. question: \" + query[0] + \"</s> question_context: \" + context\n",
        "\n",
        "    input_tokenized = FT_MODEL_TOKENIZER.encode(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=1024).to(device)\n",
        "    _tok_count_assessment = FT_MODEL_TOKENIZER.encode(input_text, return_tensors='pt', truncation=True).to(device)\n",
        "\n",
        "    summary_ids = FT_MODEL.generate(input_tokenized,\n",
        "                                       max_length=500,\n",
        "                                       min_length=25,\n",
        "                                       num_beams=2,\n",
        "                                       early_stopping=True,\n",
        "                                   )\n",
        "    output = [FT_MODEL_TOKENIZER.decode(id, clean_up_tokenization_spaces=True, skip_special_tokens=True) for id in summary_ids]\n",
        "\n",
        "    return str(output[0])\n",
        "\n",
        "device = [0 if torch.cuda.is_available() else 'cpu'][0]\n",
        "print('Answer: ')\n",
        "print(_generate(question, generatedContext, model=\"consciousAI/question-answering-generative-t5-v1-base-s-q-c\", device=device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bj-a6PU3E6H",
        "outputId": "7ac61dfe-e6d6-4be2-9e18-d101d0218e0a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \n",
            "Dr. Joshua Engelsma, is an Assistant Professor of Computing in the area of Automated Fingerprint Identification Systems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECOND APPROACH USING bert-large model"
      ],
      "metadata": {
        "id": "tRP0TfsxbSo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "nlp = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Get an answer\n",
        "question_ask = \"Answer this question based on the context and give a detailed answer. question: \"+question[0]\n",
        "answer = nlp(context=generatedContext, question=question_ask)\n",
        "\n",
        "# Display the answer and confidence score\n",
        "print(\"Answer:\", answer['answer'])\n",
        "print(\"Confidence Score:\", answer['score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGXG7u_cuW1h",
        "outputId": "37bd08bd-9fb1-4f44-db9e-e9af2f259d42"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Dr\n",
            "Confidence Score: 0.16120602190494537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIRD APPROACH USES facebook/bart-large-cnn model"
      ],
      "metadata": {
        "id": "wXxmr166bzla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load the pre-trained BART model and tokenizer\n",
        "model_name2 = \"facebook/bart-large-cnn\"\n",
        "model2 = BartForConditionalGeneration.from_pretrained(model_name2)\n",
        "tokenizer2 = BartTokenizer.from_pretrained(model_name2)\n",
        "\n",
        "\n",
        "# Convert the question-answering task into a text generation task\n",
        "input_text = f\"Answer the question from the context and do not include information unrelated to the question. Question: {question[0]} Context: {generatedContext}\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer2.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "# Generate the answer as a summary\n",
        "summary_ids = model2.generate(input_ids, max_length= 500, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "\n",
        "# Decode the generated answer\n",
        "answer = tokenizer2.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated answer\n",
        "print(\"Answer:\")\n",
        "\n",
        "print(answer.strip('\"Answer the question from the context and do not include information unrelated to the question.'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGXfqUwq5CU4",
        "outputId": "3b02498c-6d01-421e-ed69-48df3d89977b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "Come and attend the talk by Dr. Joshua Engelsma, where he sheds light on the recent trends and challenges in the area of Automated Fingerprint Identification Systems (AFIS) Dr. Zachary DeBruine is leading research in collaboration with academic and industry partners to develop high-performance machine learning algorithms to analyze big biolog\n"
          ]
        }
      ]
    }
  ]
}